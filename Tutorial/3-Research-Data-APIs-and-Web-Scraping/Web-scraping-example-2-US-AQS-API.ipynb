{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42025dc9",
   "metadata": {},
   "source": [
    "# Web scraping example 2: Downloading air quality data from US AQS API\n",
    "\n",
    "\n",
    "## Air quality data in the US\n",
    "The United States Environmental Protection Agency (US EPA) has provided multiple ways to access air quality data in the US. You can have a look at the below pages for some background information:\n",
    "\n",
    "- [US EPA Outdoor Air Quality Data Homepage](https://www.epa.gov/outdoor-air-quality-data)\n",
    "- [Map of monitoring sites](https://gispub.epa.gov/airnow/?mlayer=ozonepm&clayer=none&panel=0%203%3E%20TROPOMI%20codes%20-%20visualisation)\n",
    "\n",
    "\n",
    "## Download data from Air Quality System (AQS) API\n",
    "Details instructions:\n",
    "- [ Air Quality System API](https://aqs.epa.gov/aqsweb/documents/data_api.html)\n",
    "\n",
    "In brief:\n",
    "- you need to get your unique \"key\" first using your registered email address\n",
    "- you need the information of your target state, county, site codes and species parameter codes\n",
    "\n",
    "You can copy and paste the example links below to your web browser to see how the API works:\n",
    "- retrieve the list of states in the US: https://aqs.epa.gov/data/api/list/states?email=test@aqs.api&key=test\n",
    "\n",
    "- retireve the list of counties in a certain state: https://aqs.epa.gov/data/api/list/countiesByState?email=test@aqs.api&key=test&state=06\n",
    "\n",
    "- retrieve the list of sites in a certain county: https://aqs.epa.gov/data/api/list/sitesByCounty?email=test@aqs.api&key=test&state=06&county=037\n",
    "\n",
    "Once you have your registered email and personal key, you can edit the below link (insert your own email and key) to get some sample data:\n",
    "\n",
    "- request data at a sample site: https://aqs.epa.gov/data/api/sampleData/bySite?email=insert_your_own_email&key=insert_your_own_key&param=42602&bdate=20190101&edate=20190101&state=06&county=037&site=2005\n",
    "\n",
    "## Now use Python to request the same data\n",
    "- I will show all the steps, but I didn't run them here because the returned results are very long. In this version, you can have a better idea of how the codes are developed.\n",
    "- You only need to provide your ```\"registered email\"``` and ```\"unique key\"``` at the beginning, then you should be able to run all the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# provide your \"registered email\" and \"unique key\"\n",
    "my_email = \"insert-your-registered-email-address\" \n",
    "my_key = \"insert-your-unique-key-obtained-from-US-AQS-API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48786939",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# load packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# now build a function to download json data from the target url\n",
    "# mearsurements are in \"Data\" (\"Header\" returns the status of this request)\n",
    "\n",
    "def get_AQS_data(download_link):\n",
    "    \"\"\"Open the target download link provided by US EPA Air Quality System (AQS) API, \n",
    "       return the data or error message (if there is no data)\"\"\"\n",
    "    response = urlopen(download_link).read().decode(\"utf-8\")\n",
    "    responseJson = json.loads(response)\n",
    "    if (len(responseJson.get(\"Data\")) == 0):\n",
    "        return responseJson.get(\"Header\")\n",
    "    else:\n",
    "        return responseJson.get(\"Data\") \n",
    "    \n",
    "# request data at the same sample site using your registered email and key\n",
    "# you can remove \"\\\" at the end and write the url in a single line\n",
    "sample_url = \"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "             \"&param=42602&bdate=20190101&edate=20190101&state=06&county=037&site=2005\"\n",
    "\n",
    "# retrive data\n",
    "sample_results = get_AQS_data(sample_url)\n",
    "display(sample_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the returned results include a list of dictionaries\n",
    "print(\"type:\",type(sample_results[0]))\n",
    "print(\"number of dictionary keys:\",len(sample_results))\n",
    "print(\"dictionary keys:\",sample_results[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first record (measurements at 00:00 on the selected day)\n",
    "display(sample_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b26f99",
   "metadata": {},
   "source": [
    "## Using this function, we can loop through a big number of air quality stations qucikly.\n",
    "\n",
    "## For example, New York City seems to be missing from the historical data files provided by US EPA. Let's do a systematic check of NO2 data in New York City to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# New York state code: 36\n",
    "# New York City county code: 061\n",
    "# NO2 species code: 42602\n",
    "\n",
    "NY_sites = get_data(\"https://aqs.epa.gov/data/api/list/sitesByCounty?email=test@aqs.api&key=test&state=36&county=061\")\n",
    "\n",
    "# There seems to be a lot of missing sites ('value_represented': None)\n",
    "display(NY_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the site code list in New York City\n",
    "NY_sites_codes = []\n",
    "\n",
    "for i in range(len(NY_sites)):\n",
    "        NY_sites_codes.append(NY_sites[i]['code']) \n",
    "        \n",
    "print(NY_sites_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46808ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the corresponding link for each site code\n",
    "NY_sites_links = []\n",
    "\n",
    "for i in range(len(NY_sites_codes)):\n",
    "    NY_sites_links.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                          \"&param=42602&bdate=20190101&edate=20190131&state=36&county=061&site=\"+str(NY_sites_codes[i]))\n",
    "\n",
    "# print out all the download links\n",
    "for i in range(len(NY_sites_links)):\n",
    "    print(NY_sites_links[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from each link\n",
    "NY_data = [get_AQS_data(link) for link in NY_sites_links]   \n",
    "\n",
    "# print out the results\n",
    "for i in range(len(NY_data)):\n",
    "    print(NY_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df09eac",
   "metadata": {},
   "source": [
    "## Using this function, we can also download data for multiple species at multiple sites all at once. Let's use Los Angeles as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# download NO2,SO2,CO,O3,PM2.5,PM10 from Los Angeles\n",
    "\n",
    "# first summarize the codes needed for input variables (parameter,bdate,edate,state code,county code,site code)\n",
    "# then store information using regular expressions\n",
    "\n",
    "import re\n",
    "\n",
    "parameters = '''\n",
    "             NO2: 42602\n",
    "             SO2: 42401\n",
    "             CO: 42101\n",
    "             O3: 44201\n",
    "             PM2.5 FRM/FEM Mass: 88101\n",
    "             PM2.5 non FRM/FEM Mass: 88502\n",
    "             PM10: 81102\n",
    "             '''\n",
    "# here create the list of species in the same order (this will be used later when assigning the output file names)\n",
    "species = ['NO2','SO2','CO','O3','PM2.5 FRM FEM Mass','PM2.5 non FRM FEM Mass','PM10'] \n",
    "\n",
    "date = '''\n",
    "       begin date: 20190101\n",
    "       end date: 20191231\n",
    "       '''\n",
    "\n",
    "state_code = '''\n",
    "             California: 06\n",
    "             '''\n",
    "\n",
    "county_code = '''\n",
    "              Los Angeles: 037\n",
    "              '''\n",
    "\n",
    "site_code = '''\n",
    "             Compton: 1302\n",
    "             Lancaster: 9033\n",
    "             North Main: 1103\n",
    "             LAX: 5005\n",
    "             Glendora: 0016\n",
    "             '''\n",
    "# once we have established the order, we only need to keep numbers on the right\n",
    "# tell Python to find \"digits\" after \": \"\n",
    "parameters = re.findall(r'(?<=:\\s)\\d+',parameters)\n",
    "date = re.findall(r'(?<=:\\s)\\d+',date)\n",
    "state_code = re.findall(r'(?<=:\\s)\\d+',state_code)\n",
    "county_code = re.findall(r'(?<=:\\s)\\d+',county_code)\n",
    "site_code = re.findall(r'(?<=:\\s)\\d+',site_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a093101",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# create download links to all species at each site\n",
    "LA_Compton = []\n",
    "LA_Lancaster = []\n",
    "LA_North_Main = []\n",
    "LA_LAX = []\n",
    "LA_Glendora = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    LA_Compton.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                      \"&param=\"+str(parameters[i])+\"&bdate=\"+str(date[0])+\"&edate=\"+str(date[1])+\"&state=\"+str(state_code[0])+ \\\n",
    "                      \"&county=\"+str(county_code[0])+\"&site=\"+str(site_code[0]))\n",
    "    LA_Lancaster.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                        \"&param=\"+str(parameters[i])+\"&bdate=\"+str(date[0])+\"&edate=\"+str(date[1])+\"&state=\"+str(state_code[0])+ \\\n",
    "                        \"&county=\"+str(county_code[0])+\"&site=\"+str(site_code[1]))\n",
    "    LA_North_Main.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                         \"&param=\"+str(parameters[i])+\"&bdate=\"+str(date[0])+\"&edate=\"+str(date[1])+\"&state=\"+str(state_code[0])+ \\\n",
    "                         \"&county=\"+str(county_code[0])+\"&site=\"+str(site_code[2]))\n",
    "    LA_LAX.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                  \"&param=\"+str(parameters[i])+\"&bdate=\"+str(date[0])+\"&edate=\"+str(date[1])+\"&state=\"+str(state_code[0])+ \\\n",
    "                  \"&county=\"+str(county_code[0])+\"&site=\"+str(site_code[3]))\n",
    "    LA_Glendora.append(\"https://aqs.epa.gov/data/api/sampleData/bySite?email=\"+str(my_email)+\"&key=\"+str(my_key)+ \\\n",
    "                       \"&param=\"+str(parameters[i])+\"&bdate=\"+str(date[0])+\"&edate=\"+str(date[1])+\"&state=\"+str(state_code[0])+ \\\n",
    "                       \"&county=\"+str(county_code[0])+\"&site=\"+str(site_code[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all species at each site in LA in 2019\n",
    "LA_Compton_results = [get_AQS_data(link) for link in LA_Compton]\n",
    "LA_Lancaster_results = [get_AQS_data(link) for link in LA_Lancaster]\n",
    "LA_North_Main_results = [get_AQS_data(link) for link in LA_North_Main]\n",
    "LA_LAX_results = [get_AQS_data(link) for link in LA_LAX]\n",
    "LA_Glendora_results = [get_AQS_data(link) for link in LA_Glendora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results and save out\n",
    "# if there is data, measurements during the samplng period are returned as a list of dictionaries\n",
    "# if there is no data, an error message is kept\n",
    "# the length will be \"1\"\n",
    "\n",
    "# now build a function to convert the list of dictionary to a pandas dataframes\n",
    "def save_raw_AQS_results_to_df(raw_AQS_data_results):\n",
    "    \"\"\"For each request, the API returns measurements of one species at one site during one sampling period.\n",
    "       Results are returned as a list of dictionaries. This function converts the results from each request \n",
    "       to a single pandas dataframe.\n",
    "    \"\"\"\n",
    "    test_data = [pd.DataFrame([data]) for data in raw_AQS_data_results]\n",
    "    test_df = pd.concat(test_data,ignore_index=True)\n",
    "    return test_df\n",
    "\n",
    "# now convert results to pandas dataframes if there is data\n",
    "\n",
    "# 1> Comton site\n",
    "LA_Compton_df = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Compton_results[i]) > 1):\n",
    "        LA_Compton_df.append(save_raw_AQS_results_to_df(LA_Compton_results[i]))\n",
    "    else:\n",
    "        LA_Compton_df.append(\"There is no observation for \"+str(species[i]))\n",
    "\n",
    "# 2> Lancaster site\n",
    "LA_Lancaster_df = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Lancaster_results[i]) > 1):\n",
    "        LA_Lancaster_df.append(save_raw_AQS_results_to_df(LA_Lancaster_results[i]))\n",
    "    else:\n",
    "        LA_Lancaster_df.append(\"There is no observation for \"+str(species[i]))\n",
    "\n",
    "# 3> North Main street site\n",
    "LA_North_Main_df = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_North_Main_results[i]) > 1):\n",
    "        LA_North_Main_df.append(save_raw_AQS_results_to_df(LA_North_Main_results[i]))\n",
    "    else:\n",
    "        LA_North_Main_df.append(\"There is no observation for \"+str(species[i]))\n",
    "\n",
    "# 4> LAX site\n",
    "LA_LAX_df = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_LAX_results[i]) > 1):\n",
    "        LA_LAX_df.append(save_raw_AQS_results_to_df(LA_LAX_results[i]))\n",
    "    else:\n",
    "        LA_LAX_df.append(\"There is no observation for \"+str(species[i]))\n",
    "\n",
    "# 5> Glendora site\n",
    "LA_Glendora_df = []\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Glendora_results[i]) > 1):\n",
    "        LA_Glendora_df.append(save_raw_AQS_results_to_df(LA_Glendora_results[i]))\n",
    "    else:\n",
    "        LA_Glendora_df.append(\"There is no observation for \"+str(species[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# output the results as csv files with unique informative names\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Compton_results[i]) > 0):\n",
    "        LA_Compton_df[i].to_csv(\"LA_Compton_2019_\"+str(species[i])+\".csv\")\n",
    "        \n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Lancaster_results[i]) > 0):\n",
    "        LA_Lancaster_df[i].to_csv(\"LA_Lancaster_2019_\"+str(species[i])+\".csv\")\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_North_Main_results[i]) > 0):\n",
    "        LA_North_Main_df[i].to_csv(\"LA_North_Main_2019_\"+str(species[i])+\".csv\")\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_LAX_results[i]) > 0):\n",
    "        LA_LAX_df[i].to_csv(\"LA_LAX_2019_\"+str(species[i])+\".csv\")\n",
    "        \n",
    "for i in range(len(parameters)):\n",
    "    if (len(LA_Glendora_results[i]) > 0):\n",
    "        LA_Glendora_df[i].to_csv(\"LA_Glendora_2019_\"+str(species[i])+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921872b2",
   "metadata": {},
   "source": [
    "## The focus here is on developing a web scraping tool. Actually you can re-write the codes and use arugments to further improve the efficiency for repeated tasks. You can use arguments to import information like \"parameter code\",\"state code\", \"county code\" and \"species code\". Check out [\"argparse\"](https://docs.python.org/3/library/argparse.html) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
